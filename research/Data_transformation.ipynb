{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\BloodCellClassification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    EOSINOPHIL_dirs: list\n",
    "    LYMPHOCYTE_dir: list\n",
    "    MONOCYTE_dirs: list\n",
    "    NEUTROPHIL_dirs: list\n",
    "    img_height: int\n",
    "    img_width: int\n",
    "    batch_size: int\n",
    "    test_size: float\n",
    "    val_size: float\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.BloodCellClassifier.constant import *\n",
    "from src.BloodCellClassifier.utils.common import read_yaml,create_directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        return DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            EOSINOPHIL_dirs=[Path(x) for x in config.EOSINOPHIL_dirs],\n",
    "            LYMPHOCYTE_dir=[Path(x) for x in config.LYMPHOCYTE_dir],\n",
    "            MONOCYTE_dirs=[Path(x) for x in config.MONOCYTE_dirs],\n",
    "            NEUTROPHIL_dirs=[Path(x) for x in config.NEUTROPHIL_dirs],\n",
    "            img_height=self.params.IMG_HEIGHT,\n",
    "            img_width=self.params.IMG_WIDTH,\n",
    "            batch_size=self.params.BATCH_SIZE,\n",
    "            test_size=self.params.TEST_SIZE,\n",
    "            val_size=self.params.VAL_SIZE,\n",
    "            seed=self.params.SEED\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BloodCellClassifier import logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.class_labels = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "    def create_dataframe(self):\n",
    "        try:\n",
    "            filepaths = []\n",
    "            labels = []\n",
    "            \n",
    "            # Map directory lists to their class labels\n",
    "            dir_class_map = {\n",
    "                'EOSINOPHIL': self.config.EOSINOPHIL_dirs,\n",
    "                'LYMPHOCYTE': self.config.LYMPHOCYTE_dir,\n",
    "                'MONOCYTE': self.config.MONOCYTE_dirs,\n",
    "                'NEUTROPHIL': self.config.NEUTROPHIL_dirs\n",
    "            }\n",
    "            \n",
    "            for class_name, dir_list in dir_class_map.items():\n",
    "                for dir_path in dir_list:\n",
    "                    for f in os.listdir(dir_path):\n",
    "                        fpath = os.path.join(dir_path, f)\n",
    "                        filepaths.append(fpath)\n",
    "                        labels.append(class_name)\n",
    "            \n",
    "            # Create dataframe similar to original code\n",
    "            Fseries = pd.Series(filepaths, name=\"filepaths\")\n",
    "            Lseries = pd.Series(labels, name=\"labels\")\n",
    "            bloodCell_df = pd.concat([Fseries, Lseries], axis=1)\n",
    "            \n",
    "            logger.info(f\"Created dataframe with {len(bloodCell_df)} samples\")\n",
    "            logger.info(\"Class distribution:\\n\" + str(bloodCell_df[\"labels\"].value_counts()))\n",
    "            \n",
    "            return bloodCell_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating dataframe: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def split_data(self, df):\n",
    "        try:\n",
    "            # Split data like in original code\n",
    "            train_images, test_images = train_test_split(\n",
    "                df, \n",
    "                test_size=self.config.test_size, \n",
    "                random_state=self.config.seed\n",
    "            )\n",
    "            train_set, val_set = train_test_split(\n",
    "                train_images, \n",
    "                test_size=self.config.val_size, \n",
    "                random_state=self.config.seed\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Train set size: {len(train_set)}\")\n",
    "            logger.info(f\"Validation set size: {len(val_set)}\")\n",
    "            logger.info(f\"Test set size: {len(test_images)}\")\n",
    "            \n",
    "            return train_set, val_set, test_images\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error splitting data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def get_data_generators(self, train_set, val_set, test_images):\n",
    "        try:\n",
    "            # Create image generator like in original code\n",
    "            image_gen = ImageDataGenerator(\n",
    "                preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            )\n",
    "            \n",
    "            train_gen = image_gen.flow_from_dataframe(\n",
    "                dataframe=train_set,\n",
    "                x_col=\"filepaths\",\n",
    "                y_col=\"labels\",\n",
    "                target_size=(self.config.img_height, self.config.img_width),\n",
    "                color_mode='rgb',\n",
    "                class_mode=\"categorical\",\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            test_gen = image_gen.flow_from_dataframe(\n",
    "                dataframe=test_images,\n",
    "                x_col=\"filepaths\",\n",
    "                y_col=\"labels\",\n",
    "                target_size=(self.config.img_height, self.config.img_width),\n",
    "                color_mode='rgb',\n",
    "                class_mode=\"categorical\",\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            val_gen = image_gen.flow_from_dataframe(\n",
    "                dataframe=val_set,\n",
    "                x_col=\"filepaths\",\n",
    "                y_col=\"labels\",\n",
    "                target_size=(self.config.img_height, self.config.img_width),\n",
    "                color_mode='rgb',\n",
    "                class_mode=\"categorical\",\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Created data generators successfully\")\n",
    "            logger.info(f\"Classes: {train_gen.class_indices}\")\n",
    "            \n",
    "            return train_gen, val_gen, test_gen\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating data generators: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-31 18:37:52,506: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-31 18:37:52,506: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-31 18:37:52,506: INFO: common: created directory at: artifacts]\n",
      "[2025-07-31 18:37:52,583: INFO: 3887899035: Created dataframe with 9957 samples]\n",
      "[2025-07-31 18:37:52,585: INFO: 3887899035: Class distribution:\n",
      "labels\n",
      "NEUTROPHIL    2499\n",
      "EOSINOPHIL    2497\n",
      "LYMPHOCYTE    2483\n",
      "MONOCYTE      2478\n",
      "Name: count, dtype: int64]\n",
      "[2025-07-31 18:37:52,593: INFO: 3887899035: Train set size: 5575]\n",
      "[2025-07-31 18:37:52,594: INFO: 3887899035: Validation set size: 1394]\n",
      "[2025-07-31 18:37:52,594: INFO: 3887899035: Test set size: 2988]\n",
      "Found 5575 validated image filenames belonging to 4 classes.\n",
      "Found 2988 validated image filenames belonging to 4 classes.\n",
      "Found 1394 validated image filenames belonging to 4 classes.\n",
      "[2025-07-31 18:37:53,571: INFO: 3887899035: Created data generators successfully]\n",
      "[2025-07-31 18:37:53,571: INFO: 3887899035: Classes: {'EOSINOPHIL': 0, 'LYMPHOCYTE': 1, 'MONOCYTE': 2, 'NEUTROPHIL': 3}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Data Transformation Pipeline\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    # Create dataframe and split data\n",
    "    bloodCell_df = data_transformation.create_dataframe()\n",
    "    train_set, val_set, test_images = data_transformation.split_data(bloodCell_df)\n",
    "    \n",
    "    # Create generators\n",
    "    train_gen, val_gen, test_gen = data_transformation.get_data_generators(\n",
    "        train_set, val_set, test_images\n",
    "    )\n",
    "    \n",
    "    # Save the split datasets (now the directory exists)\n",
    "    train_set.to_csv(os.path.join(data_transformation_config.root_dir, \"train_set.csv\"), index=False)\n",
    "    val_set.to_csv(os.path.join(data_transformation_config.root_dir, \"val_set.csv\"), index=False)\n",
    "    test_images.to_csv(os.path.join(data_transformation_config.root_dir, \"test_images.csv\"), index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error in data transformation pipeline: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
