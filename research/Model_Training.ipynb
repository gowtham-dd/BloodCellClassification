{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data Science\\\\END to END Proj\\\\BloodCellClassification'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    tensorboard_log_dir: Path\n",
    "    epochs: int\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    img_height: int\n",
    "    img_width: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.BloodCellClassifier.constant import *\n",
    "from src.BloodCellClassifier.utils.common import read_yaml,create_directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        params = self.params\n",
    "\n",
    "        return ModelTrainingConfig(\n",
    "        root_dir=Path(config.root_dir),\n",
    "        trained_model_path=Path(config.trained_model_path),\n",
    "        tensorboard_log_dir=Path(config.tensorboard_log_dir),\n",
    "        epochs=params.EPOCHS,\n",
    "        learning_rate=params.LEARNING_RATE,\n",
    "        batch_size=params.BATCH_SIZE,\n",
    "        img_height=params.IMG_HEIGHT,\n",
    "        img_width=params.IMG_WIDTH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler\n",
    "from BloodCellClassifier import logger\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.tensorboard_log_dir, exist_ok=True)\n",
    "\n",
    "    def _model_exists(self):\n",
    "        \"\"\"Check if model is already saved in TF2.12 format\"\"\"\n",
    "        required_files = [\n",
    "            os.path.join(self.config.trained_model_path, \"saved_model.pb\"),\n",
    "            os.path.join(self.config.trained_model_path, \"variables/variables.index\")\n",
    "        ]\n",
    "        return all(os.path.exists(f) for f in required_files)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds the CNN model (same as original code)\"\"\"\n",
    "        model = Sequential([\n",
    "            Conv2D(128, (8, 8), strides=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Conv2D(256, (5, 5), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((3, 3)),\n",
    "            \n",
    "            Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(256, (1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(256, (1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            \n",
    "            Conv2D(512, (3, 3), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Conv2D(512, (3, 3), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(512, (3, 3), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Conv2D(512, (3, 3), activation='relu', padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(1024, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1024, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=SGD(learning_rate=self.config.learning_rate),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, train_gen, val_gen):\n",
    "        \"\"\"Trains only if model doesn't exist\"\"\"\n",
    "        if self._model_exists():\n",
    "            logger.info(f\"Model already exists at {self.config.trained_model_path}. Loading...\")\n",
    "            return load_model(self.config.trained_model_path), None\n",
    "        \n",
    "        model = self.build_model()\n",
    "        \n",
    "        callbacks = [\n",
    "            TensorBoard(log_dir=self.config.tensorboard_log_dir),\n",
    "            EarlyStopping(patience=3, restore_best_weights=True),\n",
    "            LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "        ]\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=self.config.epochs,\n",
    "            validation_data=val_gen,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return model, history\n",
    "\n",
    "    def save_model(self, model):\n",
    "        \"\"\"Saves in TF2.12 format (creates .pb + assets/ + variables/)\"\"\"\n",
    "        if self._model_exists():\n",
    "            logger.info(\"Model already exists. Skipping save.\")\n",
    "            return\n",
    "            \n",
    "        model.save(\n",
    "            self.config.trained_model_path,\n",
    "            save_format=\"tf\"  # This creates the full TF2.12 structure\n",
    "        )\n",
    "        logger.info(f\"Model saved in TF2.12 format at: {self.config.trained_model_path}\")\n",
    "        logger.info(\"Contains: saved_model.pb, assets/, variables/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    EOSINOPHIL_dirs: list\n",
    "    LYMPHOCYTE_dir: list\n",
    "    MONOCYTE_dirs: list\n",
    "    NEUTROPHIL_dirs: list\n",
    "    img_height: int\n",
    "    img_width: int\n",
    "    batch_size: int\n",
    "    test_size: float\n",
    "    val_size: float\n",
    "    seed: int\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        return DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            EOSINOPHIL_dirs=[Path(x) for x in config.EOSINOPHIL_dirs],\n",
    "            LYMPHOCYTE_dir=[Path(x) for x in config.LYMPHOCYTE_dir],\n",
    "            MONOCYTE_dirs=[Path(x) for x in config.MONOCYTE_dirs],\n",
    "            NEUTROPHIL_dirs=[Path(x) for x in config.NEUTROPHIL_dirs],\n",
    "            img_height=self.params.IMG_HEIGHT,\n",
    "            img_width=self.params.IMG_WIDTH,\n",
    "            batch_size=self.params.BATCH_SIZE,\n",
    "            test_size=self.params.TEST_SIZE,\n",
    "            val_size=self.params.VAL_SIZE,\n",
    "            seed=self.params.SEED\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-31 18:50:20,812: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-31 18:50:20,812: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-31 18:50:20,812: INFO: common: created directory at: artifacts]\n",
      "[2025-07-31 18:50:20,830: INFO: 2649934531: Creating dataframe from image directories...]\n",
      "[2025-07-31 18:50:20,898: INFO: Data_Transformation: Created dataframe with 9957 samples]\n",
      "[2025-07-31 18:50:20,902: INFO: Data_Transformation: Class distribution:\n",
      "labels\n",
      "NEUTROPHIL    2499\n",
      "EOSINOPHIL    2497\n",
      "LYMPHOCYTE    2483\n",
      "MONOCYTE      2478\n",
      "Name: count, dtype: int64]\n",
      "[2025-07-31 18:50:20,903: INFO: 2649934531: Splitting data into train/val/test sets...]\n",
      "[2025-07-31 18:50:20,911: INFO: Data_Transformation: Train set size: 5575]\n",
      "[2025-07-31 18:50:20,911: INFO: Data_Transformation: Validation set size: 1394]\n",
      "[2025-07-31 18:50:20,912: INFO: Data_Transformation: Test set size: 2988]\n",
      "[2025-07-31 18:50:20,913: INFO: 2649934531: Creating data generators...]\n",
      "Found 5575 validated image filenames belonging to 4 classes.\n",
      "Found 2988 validated image filenames belonging to 4 classes.\n",
      "Found 1394 validated image filenames belonging to 4 classes.\n",
      "[2025-07-31 18:50:23,025: INFO: Data_Transformation: Created data generators successfully]\n",
      "[2025-07-31 18:50:23,025: INFO: Data_Transformation: Classes: {'EOSINOPHIL': 0, 'LYMPHOCYTE': 1, 'MONOCYTE': 2, 'NEUTROPHIL': 3}]\n",
      "[2025-07-31 18:50:23,025: INFO: 2649934531: Saving split datasets...]\n",
      "[2025-07-31 18:50:23,142: INFO: 2649934531: Data transformation completed successfully!]\n"
     ]
    }
   ],
   "source": [
    "# Data Transformation Pipeline\n",
    "from BloodCellClassifier.components.Data_Transformation import DataTransformation\n",
    "from BloodCellClassifier import logger\n",
    "\n",
    "try:\n",
    "    # Initialize configuration\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    \n",
    "    # Initialize data transformation\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    # Step 1: Create dataframe from image directories\n",
    "    logger.info(\"Creating dataframe from image directories...\")\n",
    "    bloodCell_df = data_transformation.create_dataframe()\n",
    "    \n",
    "    # Step 2: Split data into train, validation, test sets\n",
    "    logger.info(\"Splitting data into train/val/test sets...\")\n",
    "    train_set, val_set, test_images = data_transformation.split_data(bloodCell_df)\n",
    "    \n",
    "    # Step 3: Create data generators\n",
    "    logger.info(\"Creating data generators...\")\n",
    "    train_gen, val_gen, test_gen = data_transformation.get_data_generators(\n",
    "        train_set, val_set, test_images\n",
    "    )\n",
    "    \n",
    "    # Optional: Save the split datasets\n",
    "    logger.info(\"Saving split datasets...\")\n",
    "    os.makedirs(data_transformation_config.root_dir, exist_ok=True)\n",
    "    train_set.to_csv(os.path.join(data_transformation_config.root_dir, \"train_set.csv\"), index=False)\n",
    "    val_set.to_csv(os.path.join(data_transformation_config.root_dir, \"val_set.csv\"), index=False)\n",
    "    test_images.to_csv(os.path.join(data_transformation_config.root_dir, \"test_images.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Data transformation completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error in data transformation pipeline: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-31 18:53:32,057: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-31 18:53:32,057: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-31 18:53:32,073: INFO: common: created directory at: artifacts]\n",
      "Epoch 1/15\n",
      "697/697 [==============================] - 3801s 5s/step - loss: 1.7528 - accuracy: 0.3464 - val_loss: 1.1015 - val_accuracy: 0.5129 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "697/697 [==============================] - 3502s 5s/step - loss: 1.1915 - accuracy: 0.4694 - val_loss: 0.9130 - val_accuracy: 0.6227 - lr: 9.5000e-04\n",
      "Epoch 3/15\n",
      "697/697 [==============================] - 3582s 5s/step - loss: 0.9635 - accuracy: 0.5812 - val_loss: 0.8042 - val_accuracy: 0.6585 - lr: 9.0250e-04\n",
      "Epoch 4/15\n",
      "697/697 [==============================] - 3519s 5s/step - loss: 0.7767 - accuracy: 0.6732 - val_loss: 0.6559 - val_accuracy: 0.7346 - lr: 8.5737e-04\n",
      "Epoch 5/15\n",
      "697/697 [==============================] - 3564s 5s/step - loss: 0.6042 - accuracy: 0.7523 - val_loss: 0.5475 - val_accuracy: 0.7633 - lr: 8.1451e-04\n",
      "Epoch 6/15\n",
      "697/697 [==============================] - 3712s 5s/step - loss: 0.4692 - accuracy: 0.8165 - val_loss: 0.4987 - val_accuracy: 0.7669 - lr: 7.7378e-04\n",
      "Epoch 7/15\n",
      "697/697 [==============================] - 3718s 5s/step - loss: 0.3512 - accuracy: 0.8606 - val_loss: 0.4514 - val_accuracy: 0.8185 - lr: 7.3509e-04\n",
      "Epoch 8/15\n",
      "697/697 [==============================] - 4373s 6s/step - loss: 0.2760 - accuracy: 0.8922 - val_loss: 0.2891 - val_accuracy: 0.8745 - lr: 6.9834e-04\n",
      "Epoch 9/15\n",
      "697/697 [==============================] - 3630s 5s/step - loss: 0.2013 - accuracy: 0.9286 - val_loss: 0.4611 - val_accuracy: 0.8121 - lr: 6.6342e-04\n",
      "Epoch 10/15\n",
      "697/697 [==============================] - 3617s 5s/step - loss: 0.1577 - accuracy: 0.9439 - val_loss: 0.2490 - val_accuracy: 0.8874 - lr: 6.3025e-04\n",
      "Epoch 11/15\n",
      "697/697 [==============================] - 4931s 7s/step - loss: 0.1279 - accuracy: 0.9573 - val_loss: 0.2382 - val_accuracy: 0.8895 - lr: 5.9874e-04\n",
      "Epoch 12/15\n",
      "697/697 [==============================] - 3623s 5s/step - loss: 0.0994 - accuracy: 0.9668 - val_loss: 0.2388 - val_accuracy: 0.8996 - lr: 5.6880e-04\n",
      "Epoch 13/15\n",
      "697/697 [==============================] - 3629s 5s/step - loss: 0.0738 - accuracy: 0.9804 - val_loss: 0.1650 - val_accuracy: 0.9354 - lr: 5.4036e-04\n",
      "Epoch 14/15\n",
      "697/697 [==============================] - 3619s 5s/step - loss: 0.0627 - accuracy: 0.9837 - val_loss: 0.1654 - val_accuracy: 0.9319 - lr: 5.1334e-04\n",
      "Epoch 15/15\n",
      "697/697 [==============================] - 3568s 5s/step - loss: 0.0530 - accuracy: 0.9858 - val_loss: 0.1481 - val_accuracy: 0.9440 - lr: 4.8767e-04\n",
      "[2025-08-01 10:33:24,882: WARNING: save: Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.]\n",
      "[2025-08-01 10:33:26,961: INFO: builder_impl: Assets written to: artifacts\\model_training\\blood_cell_model\\assets]\n",
      "[2025-08-01 10:33:27,143: INFO: 281479759: Model saved in TF2.12 format at: artifacts\\model_training\\blood_cell_model]\n",
      "[2025-08-01 10:33:27,143: INFO: 281479759: Contains: saved_model.pb, assets/, variables/]\n",
      "374/374 [==============================] - 461s 1s/step - loss: 0.1331 - accuracy: 0.9501\n",
      "[2025-08-01 10:41:08,077: INFO: 2929818896: Test Accuracy: 95.01%]\n"
     ]
    }
   ],
   "source": [
    "# Model Training Pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    \n",
    "    # Initialize model trainer\n",
    "    model_trainer = ModelTrainer(config=model_training_config)\n",
    "    \n",
    "    # Train the model (using the generators from transformation)\n",
    "    trained_model, history = model_trainer.train(train_gen, val_gen)\n",
    "    \n",
    "    # Save the model\n",
    "    model_trainer.save_model(trained_model)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if trained_model:  # Only evaluate if we have a model\n",
    "        test_loss, test_acc = trained_model.evaluate(test_gen)\n",
    "        logger.info(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error in training pipeline: {e}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
